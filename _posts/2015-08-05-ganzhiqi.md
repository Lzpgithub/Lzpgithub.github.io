---
layout: post
title: 感知器
categories: 搜索引擎和机器学习
description: 感知器 
keywords: 感知器
---

#### 感知器定义

所谓感知器，**就是二类分类的线性分类模型，其输入为样本的特征向量，输出为样本的类别，取+1和-1二值，即通过某样本的特征，就可以准确判断该样本属于哪一类**。顾名思义，感知器能够解决的问题首先要求特征空间是线性可分的，再者是二类分类，即将样本分为{+1，-1}两类。

从比较学术的层面来说，由输入空间到输出空间的函数：**f(x)=w.x+b**，w和b为感知器参数，w为权值（weight），b为偏置（bias）。感知器的定义中，线性方程w.x+b=0对应于问题空间中的一个超平面s，位于这个超平面两侧的样本分别被归为两类，例如下图中，红色作为一类，蓝色作为另一类，它们的特征很简单，就是它们的坐标。

![](/images/posts/MachineLearning/1.png)

作为监督学习的一种方法，感知器学习由训练样本集合X求得感知器模型，即求得模型参数w和b，这里x和y分别是特征向量和类别（也称为目标）。基于此，感知器模型可以对新的的输入样本进行分类。

假设使用Xi来表示某个样本，xi表示样本Xi其中的一个分量，一共有n给维度，则可得：

![](/images/posts/MachineLearning/2.png)

感知器的模型也可以表示为：

![](/images/posts/MachineLearning/3.png)

权值w表示该感知器的参数：

![](/images/posts/MachineLearning/4.png)

感知器对应于一个n维空间中的超平面：

![](/images/posts/MachineLearning/5.png)

它能够分类两类样本，对应超平面一侧的输入样本输出1，对应另一侧样本输出为-1。训练过程就是调整权值w，使得感知器对于两类样本分别输出1和-1.

也可将感知器推广成为线性单元，进而拟合直线进行线性回归处理：

![](/images/posts/MachineLearning/6.png)

#### 感知器训练

假设有一个感知器f（x）和训练样本集合X，我们一般通过训练样本X来调整感知器f（x）中的参数权值W。具体的做法是设计目标函数，通过梯度下降法来估计感知器参数值。

平方误差准则：衡量在当前权值向量W下感知器相对于训练样本的训练误差为：

![](/images/posts/MachineLearning/7.png)

其中D表示训练样本集合，d是训练样本集合中的一个样本，td是训练样本d的目标输出（比如-1或者1），Od是训练样本d在感知器中实际输出。E是目标输出td和实际输出Od的差的平方在所有训练样本上求和的1/2倍。加1/2是为了推导更加简洁，与求导后平方项产生因子2抵消。

E是关于W的函数，要使得E变得最小，必须找到最优解W。我们从任意的初始权值向量W0开始，然后以很小的步长反复修改这个权值向量W，而每一步修改都能是误差E减小，直到找到全局最小值点W，一个合理的选择就是找到目前最陡峭的函数E下降方向，也就是E的梯度负方向：

![](/images/posts/MachineLearning/8.png)

其中e为梯度的下降步长，写成分量形式为：

![](/images/posts/MachineLearning/9.png)

推导到这里，关键是如何计算E对wi的偏导数，则有：

![](/images/posts/MachineLearning/10.png)

其中xid表示训练样本d的一个输入分量xi，因为：

![](/images/posts/MachineLearning/11.png)

则有：

![](/images/posts/MachineLearning/12.png)

![](/images/posts/MachineLearning/13.png)

在每次迭代过程中按照关于E的梯度来改变权值，迭代完成一轮所有训练样本时，这些权值更新的序列给出了对原误差函数E的标准梯度下降的一个合理近似：

![](/images/posts/MachineLearning/14.png)

感知器是一个简单的二类分类的线性分类模型，要求我们的样本时线性可分的，什么样的样本时线性可分的呢？举例来说，在二维平面中，可以使用一条直线将+1类和-1类完美分开，那么这个样本空间就是线性可分的。如下图中的样本就是线性不可分的，感知器就不能处理这种情况。因此，感知器的所有问题都基于一个前提，就是问题空间线性可分。

![](/images/posts/MachineLearning/15.png)

#### 实验

```cpp
%感知器分类器算法
clear
%x1样本初始化 一共45个点
x1(1,1)=5.1418; x1(1,2)=0.5950;
x1(2,1)=5.5519; x1(2,2)=3.5091;
x1(3,1)=5.3836; x1(3,2)=2.8033;
x1(4,1)=3.2419; x1(4,2)=3.7278;
x1(5,1)=4.4427; x1(5,2)=3.8981;
x1(6,1)=4.9111; x1(6,2)=2.8710;
x1(7,1)=2.9259; x1(7,2)=3.4879;
x1(8,1)=4.2018; x1(8,2)=2.4973;
x1(9,1)=4.7629; x1(9,2)=2.5163;
x1(10,1)=2.7118; x1(10,2)=2.4264;
x1(11,1)=3.0470; x1(11,2)=1.5699;
x1(12,1)=4.7782; x1(12,2)=3.3504;
x1(13,1)=3.9937; x1(13,2)=4.8529;
x1(14,1)=4.5245; x1(14,2)=2.1322; 
x1(15,1)=5.3643; x1(15,2)=2.2477;
x1(16,1)=4.4820; x1(16,2)=4.0843;
x1(17,1)=3.2129; x1(17,2)=3.0592;
x1(18,1)=4.7520; x1(18,2)=5.3119;
x1(19,1)=3.8331; x1(19,2)=0.4484;
x1(20,1)=3.1838; x1(20,2)=1.4494;
x1(21,1)=6.0941; x1(21,2)=1.8544;
x1(22,1)=4.0802; x1(22,2)=6.2646;
x1(23,1)=3.0627; x1(23,2)=3.6474;
x1(24,1)=4.6357; x1(24,2)=2.3344;
x1(25,1)=5.6820; x1(25,2)=3.0450;
x1(26,1)=4.5936; x1(26,2)=2.5265;
x1(27,1)=4.7902; x1(27,2)=4.4668;
x1(28,1)=4.1053; x1(28,2)=3.0274;
x1(29,1)=3.8414; x1(29,2)=4.2269;
x1(30,1)=4.8709; x1(30,2)=4.0535;
x1(31,1)=3.8052; x1(31,2)=2.6531;
x1(32,1)=4.0755; x1(32,2)=2.8295; 
x1(33,1)=3.4734; x1(33,2)=3.1919;
x1(34,1)=3.3145; x1(34,2)=1.8009;
x1(35,1)=3.7316; x1(35,2)=2.6421;
x1(36,1)=2.8117; x1(36,2)=2.8658;
x1(37,1)=4.2486; x1(37,2)=1.4651;
x1(39,1)=3.9590; x1(39,2)=1.3024;
x1(40,1)=1.7524; x1(40,2)=1.9339;
x1(41,1)=3.4892; x1(41,2)=1.2457;
x1(42,1)=4.2492; x1(42,2)=4.5982;
x1(43,1)=4.3692; x1(43,2)=1.9794;
x1(44,1)=4.1792; x1(44,2)=0.4113;
x1(45,1)=3.9627; x1(45,2)=4.2198;

%x2样本初始化 一共55个点
x2(1,1)=9.7302; x2(1,2)=5.5080; 
x2(2,1)=8.8067; x2(2,2)=5.1319; 
x2(3,1)=8.1664; x2(3,2)=5.2801; 
x2(4,1)=6.9686; x2(4,2)=4.0172; 
x2(5,1)=7.0973; x2(5,2)=4.0559; 
x2(6,1)=9.4755; x2(6,2)=4.9869; 
x2(7,1)=9.3809; x2(7,2)=5.3543; 
x2(8,1)=7.2704; x2(8,2)=4.1053; 
x2(9,1)=8.9674; x2(9,2)=5.8121; 
x2(10,1)=8.2606; x2(10,2)=5.1095; 
x2(11,1)=7.5518; x2(11,2)=7.7316; 
x2(12,1)=7.0016; x2(12,2)=5.4111; 
x2(13,1)=8.3442; x2(13,2)=3.6931; 
x2(14,1)=5.8173; x2(14,2)=5.3838; 
x2(15,1)=6.1123; x2(15,2)=5.4995; 
x2(16,1)=10.4188;x2(16,2)=4.4892; 
x2(17,1)=7.9136; x2(17,2)=5.2349; 
x2(18,1)=11.1547;x2(18,2)=4.4022; 
x2(19,1)=7.7080; x2(19,2)=5.0208; 
x2(20,1)=8.2079; x2(20,2)=5.4194; 
x2(21,1)=9.1078; x2(21,2)=6.1911; 
x2(22,1)=7.7857; x2(22,2)=5.7712; 
x2(23,1)=7.3740; x2(23,2)=2.3558; 
x2(24,1)=9.7184; x2(24,2)=5.2854; 
x2(25,1)=6.9559; x2(25,2)=5.8261; 
x2(26,1)=8.9691; x2(26,2)=4.9919; 
x2(27,1)=7.3872; x2(27,2)=5.8584; 
x2(28,1)=8.8922; x2(28,2)=5.7748; 
x2(29,1)=9.0175; x2(29,2)=6.3059; 
x2(30,1)=7.0041; x2(30,2)=6.2315; 
x2(31,1)=8.6396; x2(31,2)=5.9586; 
x2(32,1)=9.2394; x2(32,2)=3.3455; 
x2(33,1)=6.7376; x2(33,2)=4.0096; 
x2(34,1)=8.4345; x2(34,2)=5.6852;
x2(35,1)=7.9559; x2(35,2)=4.0251; 
x2(36,1)=6.5268; x2(36,2)=4.3933; 
x2(37,1)=7.6699; x2(37,2)=5.6868; 
x2(38,1)=7.8075; x2(38,2)=5.0200; 
x2(39,1)=6.6997; x2(39,2)=6.0638; 
x2(40,1)=5.6549; x2(40,2)=3.6590; 
x2(41,1)=6.9086; x2(41,2)=5.4795; 
x2(42,1)=7.9933; x2(42,2)=3.3660; 
x2(43,1)=5.9318; x2(43,2)=3.5573; 
x2(44,1)=9.5157; x2(44,2)=5.2938; 
x2(45,1)=7.2795; x2(45,2)=4.8596; 
x2(46,1)=5.5233; x2(46,2)=3.8697; 
x2(47,1)=8.1331; x2(47,2)=4.7075; 
x2(48,1)=9.7851; x2(48,2)=4.4175; 
x2(49,1)=8.0636; x2(49,2)=4.1037; 
x2(50,1)=8.1944; x2(50,2)=5.2486; 
x2(51,1)=7.9677; x2(51,2)=3.5103; 
x2(52,1)=8.2083; x2(52,2)=5.3135; 
x2(53,1)=9.0586; x2(53,2)=2.9749; 
x2(54,1)=8.2188; x2(54,2)=5.5290; 
x2(55,1)=8.9064; x2(55,2)=5.3435;

plot(x1(:,1),x1(:,2),'*',x2(:,1),x2(:,2),'o');

x=[x1;x2];               %创建x设计矩阵 100行2列 前45行是x1 后55行是x2
w0=0.00001;  w1=0.00001;  w2=0.00001;    %权值初始化
n=0.0005;                %梯度下降幅度
y1=-ones(45,1);          %y1类的期望输出为-1 
y2=ones(55,1);           %y2类的期望输出为1
y=[y1;y2];               %目标向量

for k=1:1000             %迭代1000次
    for i=1:100
        o=w0+w1*x(i,1)+w2*x(i,2);
        dlta=n*(y(i)-o);
        w0=w0+dlta;      %更新权值
        w1=w1+dlta*x(i,1);
        w2=w2+dlta*x(i,2);
    end
end
w0
w1
w2

xx=linspace(0,10,5000); 
yy=-w1/w2*xx-w0/w2; 
hold on,plot(xx,yy,'r');
```

程序运行结果：

![](/images/posts/MachineLearning/16.png)